{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310ae641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "def extract_policy_blocks(text, lines_after=1):\n",
    "    policy_keywords = [\n",
    "        \"사업명\", \"사업개요\", \"사업목적\", \"지원대상\", \"대상자\", \"신청대상\", \"자격요건\",\n",
    "        \"신청방법\", \"지원내용\", \"지원규모\", \"융자조건\", \"대출조건\", \"금리\", \"접수기간\", \"문의처\"\n",
    "    ]\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    result = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        if any(keyword in line for keyword in policy_keywords):\n",
    "            block = [line]\n",
    "            # 뒤에 설명줄도 같이 포함\n",
    "            for j in range(1, lines_after + 1):\n",
    "                if i + j < len(lines):\n",
    "                    block.append(lines[i + j].strip())\n",
    "            result.append(\"\\n\".join(block))\n",
    "            i += lines_after  # 다음 줄로 건너뜀\n",
    "        i += 1\n",
    "\n",
    "    return \"\\n\\n\".join(result)\n",
    "\n",
    "# 크롬 드라이버 설정\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # 브라우저 안 띄우고 실행\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# URL 접속\n",
    "url = \"https://housing.seoul.go.kr/site/main/content/sh01_070400/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "# 렌더링 완료된 HTML 가져오기\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 주요 콘텐츠 추출\n",
    "main = soup.find(\"div\", class_=\"sh-view\") or soup.find(\"div\", class_=\"content\")\n",
    "text = main.get_text(separator='\\n') if main else soup.get_text()\n",
    "text = re.sub(r\"\\s+\", \"\\n\", text).strip()\n",
    "\n",
    "\n",
    "policy_only = extract_policy_blocks(text, lines_after=10)\n",
    "\n",
    "with open(\"정책요약.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(policy_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34458ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: 표_스크래핑_txt\\https___housing_seoul_go_kr_site_main_content_sh01_070400_.txt\n",
      "✅ 저장 완료: 표_스크래핑_txt\\sh01_060513.txt\n",
      "✅ 저장 완료: 표_스크래핑_txt\\sh01_040901.txt\n",
      "✅ 저장 완료: 표_스크래핑_txt\\sh01_060508.txt\n",
      "✅ 저장 완료: 표_스크래핑_txt\\sh01_060503.txt\n",
      "✅ 저장 완료: 표_스크래핑_txt\\sh01_060506.txt\n",
      "✅ 저장 완료: 표_스크래핑_txt\\menu_es_mid_a10401010000.txt\n",
      "✅ 저장 완료: 표_스크래핑_txt\\menu_es_mid_a10401020100.txt\n",
      "✅ 저장 완료: 표_스크래핑_txt\\menu_es_mid_a10401020200.txt\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "# URL 리스트\n",
    "urls = [\n",
    "    \"https://housing.seoul.go.kr/site/main/content/sh01_070400/\",\n",
    "    \"https://housing.seoul.go.kr/site/main/content/sh01_060513\",\n",
    "    \"https://housing.seoul.go.kr/site/main/content/sh01_040901\",\n",
    "    \"https://housing.seoul.go.kr/site/main/content/sh01_060508\",\n",
    "    \"https://housing.seoul.go.kr/site/main/content/sh01_060503\",\n",
    "    \"https://housing.seoul.go.kr/site/main/content/sh01_060506\",\n",
    "    \"https://www.lh.or.kr/menu.es?mid=a10401010000\",\n",
    "    \"https://www.lh.or.kr/menu.es?mid=a10401020100\",\n",
    "    \"https://www.lh.or.kr/menu.es?mid=a10401020200\"\n",
    "]\n",
    "\n",
    "# 브라우저 옵션 설정 (headless)\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# 저장 폴더 생성\n",
    "save_folder = \"표_스크래핑_txt\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# 크롤링 및 저장\n",
    "for idx, url in enumerate(urls):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "\n",
    "    result_lines = []\n",
    "    for table in tables:\n",
    "        for row in table.find_all('tr'):\n",
    "            cols = row.find_all(['th', 'td'])\n",
    "            line = \" | \".join(col.get_text(strip=True) for col in cols)\n",
    "            if line:\n",
    "                result_lines.append(line)\n",
    "        result_lines.append(\"-\" * 60)\n",
    "\n",
    "    # 파일 이름 정리 (URL에서 이름 따기)\n",
    "    name_hint = re.sub(r\"[^a-zA-Z0-9가-힣]\", \"_\", url.split(\"/\")[-1] or url.split(\"=\")[-1])\n",
    "    filename = f\"{name_hint}.txt\"\n",
    "    filepath = os.path.join(save_folder, filename)\n",
    "\n",
    "    # 저장\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[출처 URL] {url}\\n\\n\")\n",
    "        f.write(\"\\n\".join(result_lines))\n",
    "\n",
    "    print(f\"✅ 저장 완료: {filepath}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dcdfd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ChromaDB에 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# 1. txt 파일 리스트\n",
    "txt_folder = \"./표_스크래핑_txt\"  # 여기에 txt 파일들 넣어둬\n",
    "file_paths = [os.path.join(txt_folder, f) for f in os.listdir(txt_folder) if f.endswith(\".txt\")]\n",
    "\n",
    "# 2. 문서 로딩\n",
    "docs = []\n",
    "for path in file_paths:\n",
    "    loader = TextLoader(path, encoding=\"utf-8\")\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "# 3. 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 4. HuggingFace 임베딩 모델\n",
    "embeddings = OpenAIEmbeddings(model = 'text-embedding-3-large')\n",
    "\n",
    "# 5. Chroma DB로 벡터화 및 저장\n",
    "chroma_path = \"./chroma_policy_db\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=chroma_path\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✅ ChromaDB에 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698e731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ 1번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_01.txt\n",
      "▶ 2번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_02.txt\n",
      "▶ 3번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_03.txt\n",
      "▶ 4번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_04.txt\n",
      "▶ 5번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_05.txt\n",
      "▶ 6번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_06.txt\n",
      "▶ 7번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_07.txt\n",
      "▶ 8번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_08.txt\n",
      "▶ 9번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_09.txt\n",
      "▶ 10번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_10.txt\n",
      "▶ 11번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_11.txt\n",
      "▶ 12번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_12.txt\n",
      "▶ 13번 URL 스크래핑 중...\n",
      "✔ 저장 완료: output/seoul_youth_13.txt\n",
      "\n",
      "✅ 모든 `.txt` 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# ▶ 저장 폴더 생성\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# ▶ 스크래핑 대상 URL 목록\n",
    "urls = [\n",
    "    \"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId=20250316005400210633\",\n",
    "    \"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId=20250316005400210632\",\n",
    "    \"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId=20250316005400210630\",\n",
    "    \"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId=20250316005400210628\",\n",
    "    \"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId=20250316005400210627\",\n",
    "    \"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId=20250316005400210626\",\n",
    "    \"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId=R2024031920824\",\n",
    "    \"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId=R2024032020863\",\n",
    "    \"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId=R2024041221755\",\n",
    "    \"https://youth.seoul.go.kr/infoData/plcyInfo/view.do?plcyBizId=R2024051022688\",\n",
    "    \"https://youth.seoul.go.kr/infoData/sprtInfo/view.do?sprtInfoId=62335\",\n",
    "    \"https://youth.seoul.go.kr/infoData/sprtInfo/view.do?sprtInfoId=62275\",\n",
    "    \"https://youth.seoul.go.kr/infoData/sprtInfo/view.do?sprtInfoId=62127\"\n",
    "]\n",
    "\n",
    "def extract_text_and_tables(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    # 본문 내용 추출\n",
    "    content_div = soup.select_one(\"div.board-detail-contents\") or soup.select_one(\"div.cont-view\")\n",
    "    content_text = content_div.get_text(separator=\"\\n\", strip=True) if content_div else \"본문 없음\"\n",
    "\n",
    "    # 표 추출\n",
    "    tables = soup.find_all(\"table\")\n",
    "    table_texts = []\n",
    "    for t_idx, table in enumerate(tables, start=1):\n",
    "        table_lines = [f\"\\n[표 {t_idx}]\\n\"]\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cols = [col.get_text(strip=True) for col in row.find_all([\"th\", \"td\"])]\n",
    "            if cols:\n",
    "                table_lines.append(\" | \".join(cols))\n",
    "        table_texts.append(\"\\n\".join(table_lines))\n",
    "\n",
    "    return content_text, table_texts\n",
    "\n",
    "# ▶ 스크래핑 및 저장 실행\n",
    "for idx, url in enumerate(urls, start=1):\n",
    "    print(f\"▶ {idx}번 URL 스크래핑 중...\")\n",
    "\n",
    "    text, table_texts = extract_text_and_tables(url)\n",
    "    base_filename = f\"output/seoul_youth_{idx:02d}.txt\"\n",
    "\n",
    "    with open(base_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[URL]\\n{url}\\n\\n\")\n",
    "        f.write(\"[본문]\\n\")\n",
    "        f.write(text + \"\\n\\n\")\n",
    "        for table_text in table_texts:\n",
    "            f.write(table_text + \"\\n\")\n",
    "\n",
    "    print(f\"✔ 저장 완료: {base_filename}\")\n",
    "\n",
    "print(\"\\n✅ 모든 `.txt` 저장 완료!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
